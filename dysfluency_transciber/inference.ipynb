{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ['HF_HOME'] = ''\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration,WhisperTokenizer\n",
    "from safetensors.torch import load_file\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "def load_audio_data(file_path):\n",
    "    audio_array, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "    return audio_array, sampling_rate\n",
    "\n",
    "\n",
    "def create_dataset(csv_file, max_samples=None):\n",
    "    processed_data = []\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    if max_samples is not None:\n",
    "        df = df.head(max_samples) \n",
    "        \n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"prcessing data\"):\n",
    "        audio_path = row['audio_path']\n",
    "        label = row['label']\n",
    "        audio_array, sampling_rate = load_audio_data(audio_path)\n",
    "        item = {\n",
    "            'audio': {\n",
    "                'path': audio_path,\n",
    "                'array': audio_array,\n",
    "                'sampling_rate': sampling_rate\n",
    "            },\n",
    "            'sentence': label\n",
    "        }\n",
    "        processed_data.append(item)\n",
    "    \n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "csv_file = f'/csv file for dataset/' \n",
    "dataset = create_dataset(csv_file, max_samples=1000)\n",
    "\n",
    "print('loading dataset....')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "index = 3\n",
    "audio_sample = dataset[index]['audio']\n",
    "text = dataset[index]['sentence']\n",
    "\n",
    "print(f\"{text}\")\n",
    "audio = np.array(audio_sample['array'])\n",
    "ipd.display(ipd.Audio(audio, rate=16000, normalize=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v3-turbo\", language=\"en\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
    "\n",
    "level = \"word\"  # \"word\" or \"phn\"\n",
    "\n",
    "if level == \"word\":\n",
    "    new_tokens = [\"[REP]\", \"[PAU]\", \"[INS]\"]\n",
    "else:\n",
    "    new_tokens = [\"[REP]\",  \"[PRO]\", \"[PAU]\"]\n",
    "\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(\"loading model weights....\")\n",
    "state_dict = load_file(f'...../model.safetensors')  # set path to your safetensors file\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = processor(\n",
    "    audio_sample['array'], sampling_rate=audio_sample[\"sampling_rate\"], return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "predicted_ids = model.generate(input_features, language='en')\n",
    "print(predicted_ids)\n",
    "\n",
    "transcription = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
